{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and save functions\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(name , 'rb') as f:\n",
    "        return pd.DataFrame(pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN on the 'model' data to predict subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix #, classification_report, f1_score, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras import backend as tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "from keras.initializers import random_uniform\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all spectra in internal memory \n",
    "locationSpectra = 'spectra_matched/'\n",
    "filenames = glob.glob(locationSpectra+'*pkl')\n",
    "\n",
    "X = np.zeros((len(filenames),5000))\n",
    "y = []\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    df_current = load_obj(filenames[i])\n",
    "    l = len(df_current['model'])\n",
    "    X[i][0:l] = df_current['model']\n",
    "    y.append(df_current['class'].iloc[0])\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y = encoder.transform(y)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "def decode(dummy_y,encoder):\n",
    "    \"\"\"\n",
    "    Function that takes the dummy variable and its encoder and transforms it\n",
    "    back to the initial form\n",
    "    \"\"\"\n",
    "    # from dummy back to class names\n",
    "    encoded_y = np.zeros(len(dummy_y))\n",
    "    for i in range(len(dummy_y)):\n",
    "        encoded_y[i] = int(np.argmax(dummy_y[i]))\n",
    "    classes_y =  encoder.inverse_transform(encoded_y.astype(int))\n",
    "    \n",
    "    return classes_y,encoded_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, dummy_y, test_size=0.1,  random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter target names \n",
    "target_names = ['GALAXY', 'QSO', 'STAR']\n",
    "numberTargets = len(target_names) # Useful later (for the final dense layer of CNN).\n",
    "\n",
    "# Reshape for CNN. (Honestly, I'm not sure why this makes a difference) \n",
    "X_train = np.reshape(X_train, (np.size(X_train,0), np.size(X_train,1), 1))\n",
    "X_test = np.reshape(X_test, (np.size(X_test,0), np.size(X_test,1), 1))\n",
    "X_val = np.reshape(X_val, (np.size(X_val,0), np.size(X_val,1), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose hyperparameters\n",
    "no_epochs = 20\n",
    "batch_size = 128\n",
    "learning_rate = 0.0100\n",
    "dropout_rate = 0.45\n",
    "\n",
    "# Design the Network\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(32, 6, activation='relu',  input_shape=X_train[0].shape)) # Input shape is VERY fiddly. May need to try different things. \n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(layers.Conv1D(64, 4, activation='relu'))\n",
    "model.add(layers.Conv1D(128, 6, activation='relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(layers.Conv1D(64, 4, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(Dense(numberTargets, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=RMSprop(lr=learning_rate),\n",
    "              loss='categorical_crossentropy', # May need to change to binary_crossentropy or categorical_crossentropy\n",
    "              metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=no_epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n",
    "predictions = model.predict_proba(X_test)\n",
    "predicted_labels = predictions.argmax(axis=1) # Converts probabilities (e.g. 0.035 0.001 0.704 0.260) to labels (e.g. 0 0 1 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation \n",
    "\n",
    "# Now our model is built and trained and tested; we look at results\n",
    "# First, the loss and accuracy on training+validation data.\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy (Unsmoothed)')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to plot smoother plots of validation and accuracy - this often helps to identify a trend better\n",
    "def smooth_curve(points, factor=0.8):\n",
    "  smoothed_points = []\n",
    "  for point in points:\n",
    "    if smoothed_points:\n",
    "      previous = smoothed_points[-1]\n",
    "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "    else:\n",
    "      smoothed_points.append(point)\n",
    "  return smoothed_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(smooth_curve(history.history['acc']), 'bo', label='Smoothed training acc', alpha=0.5)\n",
    "plt.plot(smooth_curve(history.history['val_acc']), 'b', label='Smoothed validation acc')\n",
    "plt.title('Training and validation Accuracy (smoothed')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(smooth_curve(history.history['loss']), 'bo', label='Smoothed training loss', alpha=0.5)\n",
    "plt.plot(smooth_curve(history.history['val_loss']), 'b', label='Smoothed validation loss')\n",
    "plt.title('Training and validation loss (Smoothed)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see the results on Test data; rather than just training and validation sets\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print('Note that in multi-class and imbalanced classification problems, test accuracy is not an ideal metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_test.argmax(axis=1), predicted_labels)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This longer code gets a prettier confusion matrix.\n",
    "def cm_analysis(y_true, y_pred, filename, labels, ymap=None, figsize=(10,10)):\n",
    "    \"\"\"\n",
    "    Generate matrix plot of confusion matrix with pretty annotations.\n",
    "    The plot image is saved to disk.\n",
    "    args: \n",
    "      y_true:    true label of the data, with shape (nsamples,)\n",
    "      y_pred:    prediction of the data, with shape (nsamples,)\n",
    "      filename:  filename of figure file to save\n",
    "      labels:    string array, name the order of class labels in the confusion matrix.\n",
    "                 use `clf.classes_` if using scikit-learn models.\n",
    "                 with shape (nclass,).\n",
    "      ymap:      dict: any -> string, length == nclass.\n",
    "                 if not None, map the labels & ys to more understandable strings.\n",
    "                 Caution: original y_true, y_pred and labels must align.\n",
    "      figsize:   the size of the figure plotted.\n",
    "    \"\"\"\n",
    "    if ymap != None:\n",
    "        y_pred = [ymap[yi] for yi in y_pred]\n",
    "        y_true = [ymap[yi] for yi in y_true]\n",
    "        labels = [ymap[yi] for yi in labels]\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(cm_perc, annot=annot, fmt='', ax=ax) # Changing cm_perc to cm give a heatmap in terms of the percentages instead of absolute number \n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test.argmax(axis=1)\n",
    "cm_analysis(y_true, predicted_labels, filename='CNN_Example.png', labels=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
